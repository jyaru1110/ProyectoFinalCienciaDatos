{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final de Ciencia de Datos\n",
    "\n",
    "Este notebook implementa un pipeline completo de ciencia de datos que incluye:\n",
    "1. **Extracción de datos**: Registro de dimensiones iniciales del dataset\n",
    "2. **Limpieza de datos**: Documentación de valores nulos, duplicados e inconsistencias en tipos de datos\n",
    "3. **Análisis Exploratorio de Datos (EDA)**: Distribuciones, frecuencias y estadísticos descriptivos\n",
    "4. **Análisis Estadístico**: Planteamiento y evaluación de hipótesis\n",
    "5. **Machine Learning**: Entrenamiento de cinco modelos de ML y evaluación de desempeño\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para análisis estadístico\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro, chi2_contingency\n",
    "\n",
    "# Para machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Configuración de visualizaciones\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracción de Datos\n",
    "\n",
    "En esta sección cargamos el dataset y registramos sus dimensiones iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset\n",
    "# NOTA: Reemplaza 'dataset.csv' con la ruta a tu dataset\n",
    "# Para este ejemplo, crearemos un dataset sintético para demostración\n",
    "\n",
    "# Crear dataset sintético para demostración\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'edad': np.random.randint(18, 80, n_samples),\n",
    "    'ingresos': np.random.normal(50000, 15000, n_samples),\n",
    "    'educacion': np.random.choice(['Secundaria', 'Universidad', 'Posgrado'], n_samples, p=[0.4, 0.4, 0.2]),\n",
    "    'experiencia_anos': np.random.randint(0, 40, n_samples),\n",
    "    'satisfaccion': np.random.randint(1, 6, n_samples),\n",
    "    'categoria_cliente': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.3, 0.5, 0.2])\n",
    "}\n",
    "\n",
    "# Introducir algunos valores nulos y duplicados para demostración\n",
    "df = pd.DataFrame(data)\n",
    "df.loc[np.random.choice(df.index, 50, replace=False), 'ingresos'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30, replace=False), 'educacion'] = np.nan\n",
    "\n",
    "# Añadir duplicados\n",
    "duplicated_rows = df.sample(20)\n",
    "df = pd.concat([df, duplicated_rows], ignore_index=True)\n",
    "\n",
    "# Para cargar un dataset real, usa:\n",
    "# df = pd.read_csv('ruta_a_tu_dataset.csv')\n",
    "\n",
    "print(\"Dataset cargado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registro de dimensiones iniciales del dataset\n",
    "print(\"=== DIMENSIONES INICIALES DEL DATASET ===\")\n",
    "print(f\"Número de filas: {df.shape[0]}\")\n",
    "print(f\"Número de columnas: {df.shape[1]}\")\n",
    "print(f\"Tamaño total del dataset: {df.shape[0] * df.shape[1]} elementos\")\n",
    "print(f\"Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== INFORMACIÓN GENERAL ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== PRIMERAS 5 FILAS ===\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de Datos\n",
    "\n",
    "Documentamos y tratamos valores nulos, duplicados e inconsistencias en tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores nulos\n",
    "print(\"=== ANÁLISIS DE VALORES NULOS ===\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "null_summary = pd.DataFrame({\n",
    "    'Valores_Nulos': null_counts,\n",
    "    'Porcentaje': null_percentages\n",
    "})\n",
    "\n",
    "print(null_summary[null_summary['Valores_Nulos'] > 0])\n",
    "\n",
    "# Visualización de valores nulos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "plt.title('Patrón de Valores Nulos en el Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de duplicados\n",
    "print(\"=== ANÁLISIS DE DUPLICADOS ===\")\n",
    "duplicated_count = df.duplicated().sum()\n",
    "print(f\"Número de filas duplicadas: {duplicated_count}\")\n",
    "print(f\"Porcentaje de duplicados: {(duplicated_count / len(df)) * 100:.2f}%\")\n",
    "\n",
    "if duplicated_count > 0:\n",
    "    print(\"\\nPrimeras 5 filas duplicadas:\")\n",
    "    display(df[df.duplicated()].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de tipos de datos e inconsistencias\n",
    "print(\"=== ANÁLISIS DE TIPOS DE DATOS ===\")\n",
    "print(\"\\nTipos de datos actuales:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== DETECCIÓN DE INCONSISTENCIAS ===\")\n",
    "\n",
    "# Verificar valores únicos en variables categóricas\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nValores únicos en '{col}':\")\n",
    "    print(df[col].value_counts(dropna=False))\n",
    "\n",
    "# Verificar rangos en variables numéricas\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "print(\"\\n=== RANGOS DE VARIABLES NUMÉRICAS ===\")\n",
    "for col in numerical_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Min: {df[col].min():.2f}\")\n",
    "    print(f\"  Max: {df[col].max():.2f}\")\n",
    "    print(f\"  Media: {df[col].mean():.2f}\")\n",
    "    print(f\"  Mediana: {df[col].median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación de transformaciones necesarias\n",
    "print(\"=== APLICANDO TRANSFORMACIONES ===\")\n",
    "\n",
    "# Crear una copia del dataset original\n",
    "df_original = df.copy()\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Eliminar duplicados\n",
    "print(f\"Eliminando {df_clean.duplicated().sum()} filas duplicadas...\")\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "# 2. Tratar valores nulos\n",
    "print(\"\\nTratando valores nulos...\")\n",
    "# Para ingresos (numérica), usar la mediana\n",
    "if 'ingresos' in df_clean.columns:\n",
    "    median_income = df_clean['ingresos'].median()\n",
    "    df_clean['ingresos'].fillna(median_income, inplace=True)\n",
    "    print(f\"  - Ingresos: {null_counts['ingresos']} valores nulos reemplazados con mediana ({median_income:.2f})\")\n",
    "\n",
    "# Para educación (categórica), usar la moda\n",
    "if 'educacion' in df_clean.columns:\n",
    "    mode_education = df_clean['educacion'].mode()[0]\n",
    "    df_clean['educacion'].fillna(mode_education, inplace=True)\n",
    "    print(f\"  - Educación: {null_counts['educacion']} valores nulos reemplazados con moda ({mode_education})\")\n",
    "\n",
    "# 3. Verificar y corregir tipos de datos\n",
    "print(\"\\nVerificando tipos de datos...\")\n",
    "# Asegurar que las variables categóricas sean de tipo 'category'\n",
    "categorical_cols = ['educacion', 'categoria_cliente']\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].astype('category')\n",
    "        print(f\"  - {col}: convertido a categórico\")\n",
    "\n",
    "# 4. Verificar rangos lógicos\n",
    "print(\"\\nVerificando rangos lógicos...\")\n",
    "# Edad debe estar entre 18 y 100\n",
    "if 'edad' in df_clean.columns:\n",
    "    outliers_edad = ((df_clean['edad'] < 18) | (df_clean['edad'] > 100)).sum()\n",
    "    print(f\"  - Edad: {outliers_edad} valores fuera del rango lógico (18-100)\")\n",
    "\n",
    "# Satisfacción debe estar entre 1 y 5\n",
    "if 'satisfaccion' in df_clean.columns:\n",
    "    outliers_sat = ((df_clean['satisfaccion'] < 1) | (df_clean['satisfaccion'] > 5)).sum()\n",
    "    print(f\"  - Satisfacción: {outliers_sat} valores fuera del rango lógico (1-5)\")\n",
    "\n",
    "print(f\"\\nDataset limpio - Filas: {df_clean.shape[0]}, Columnas: {df_clean.shape[1]}\")\n",
    "print(f\"Filas eliminadas en el proceso: {df_original.shape[0] - df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "Exploramos las distribuciones, frecuencias y estadísticos descriptivos de nuestras variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticos descriptivos\n",
    "print(\"=== ESTADÍSTICOS DESCRIPTIVOS ===\")\n",
    "print(\"\\nVariables numéricas:\")\n",
    "display(df_clean.describe())\n",
    "\n",
    "print(\"\\nVariables categóricas:\")\n",
    "categorical_stats = df_clean.select_dtypes(include=['category', 'object']).describe()\n",
    "if not categorical_stats.empty:\n",
    "    display(categorical_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuciones de variables numéricas\n",
    "print(\"=== DISTRIBUCIONES DE VARIABLES NUMÉRICAS ===\")\n",
    "\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "n_cols = len(numerical_cols)\n",
    "n_rows = (n_cols + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    # Histograma\n",
    "    axes[i].hist(df_clean[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'Distribución de {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "    \n",
    "    # Añadir línea de media\n",
    "    mean_val = df_clean[col].mean()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', \n",
    "                   label=f'Media: {mean_val:.2f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Ocultar ejes vacíos\n",
    "for i in range(len(numerical_cols), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots para detectar outliers\n",
    "print(\"=== DETECCIÓN DE OUTLIERS ===\")\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    df_clean.boxplot(column=col, ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot - {col}')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "# Ocultar ejes vacíos\n",
    "for i in range(len(numerical_cols), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frecuencias de variables categóricas\n",
    "print(\"=== FRECUENCIAS DE VARIABLES CATEGÓRICAS ===\")\n",
    "\n",
    "categorical_cols = df_clean.select_dtypes(include=['category', 'object']).columns\n",
    "n_cat_cols = len(categorical_cols)\n",
    "\n",
    "if n_cat_cols > 0:\n",
    "    fig, axes = plt.subplots(1, n_cat_cols, figsize=(6*n_cat_cols, 6))\n",
    "    if n_cat_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        # Contar frecuencias\n",
    "        value_counts = df_clean[col].value_counts()\n",
    "        \n",
    "        # Gráfico de barras\n",
    "        value_counts.plot(kind='bar', ax=axes[i], rot=45)\n",
    "        axes[i].set_title(f'Frecuencias - {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frecuencia')\n",
    "        \n",
    "        # Añadir valores en las barras\n",
    "        for j, v in enumerate(value_counts.values):\n",
    "            axes[i].text(j, v + 0.1, str(v), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar tablas de frecuencias\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nTabla de frecuencias - {col}:\")\n",
    "        freq_table = df_clean[col].value_counts()\n",
    "        freq_percentage = df_clean[col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        freq_df = pd.DataFrame({\n",
    "            'Frecuencia': freq_table,\n",
    "            'Porcentaje': freq_percentage.round(2)\n",
    "        })\n",
    "        display(freq_df)\n",
    "else:\n",
    "    print(\"No hay variables categóricas en el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "print(\"=== MATRIZ DE CORRELACIÓN ===\")\n",
    "\n",
    "# Calcular correlaciones solo para variables numéricas\n",
    "numeric_df = df_clean.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Crear heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Matriz de Correlación entre Variables Numéricas')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar correlaciones fuertes\n",
    "print(\"\\nCorrelaciones más fuertes (|r| > 0.5):\")\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "correlation_matrix_masked = correlation_matrix.mask(mask)\n",
    "\n",
    "strong_correlations = []\n",
    "for i in range(len(correlation_matrix_masked.columns)):\n",
    "    for j in range(len(correlation_matrix_masked.columns)):\n",
    "        val = correlation_matrix_masked.iloc[i, j]\n",
    "        if pd.notna(val) and abs(val) > 0.5:\n",
    "            strong_correlations.append((\n",
    "                correlation_matrix_masked.columns[i],\n",
    "                correlation_matrix_masked.columns[j],\n",
    "                val\n",
    "            ))\n",
    "\n",
    "if strong_correlations:\n",
    "    for var1, var2, corr in strong_correlations:\n",
    "        print(f\"  {var1} - {var2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"  No se encontraron correlaciones fuertes (|r| > 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis Estadístico\n",
    "\n",
    "Planteamos y evaluamos hipótesis estadísticas sobre nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ANÁLISIS ESTADÍSTICO - PLANTEAMIENTO DE HIPÓTESIS ===\")\n",
    "\n",
    "# Hipótesis 1: Los ingresos siguen una distribución normal\n",
    "print(\"\\n--- HIPÓTESIS 1: Normalidad de los Ingresos ---\")\n",
    "print(\"H0: Los ingresos siguen una distribución normal\")\n",
    "print(\"H1: Los ingresos NO siguen una distribución normal\")\n",
    "\n",
    "# Test de normalidad\n",
    "ingresos_data = df_clean['ingresos'].dropna()\n",
    "\n",
    "# Shapiro-Wilk test (para muestras < 5000)\n",
    "if len(ingresos_data) < 5000:\n",
    "    stat_shapiro, p_shapiro = shapiro(ingresos_data)\n",
    "    print(f\"\\nShapiro-Wilk Test:\")\n",
    "    print(f\"  Estadístico: {stat_shapiro:.4f}\")\n",
    "    print(f\"  p-valor: {p_shapiro:.4f}\")\n",
    "    \n",
    "    if p_shapiro < 0.05:\n",
    "        print(f\"  Resultado: RECHAZAMOS H0 (p < 0.05)\")\n",
    "        print(f\"  Conclusión: Los ingresos NO siguen una distribución normal\")\n",
    "    else:\n",
    "        print(f\"  Resultado: NO rechazamos H0 (p ≥ 0.05)\")\n",
    "        print(f\"  Conclusión: No hay evidencia suficiente para rechazar la normalidad\")\n",
    "\n",
    "# D'Agostino and Pearson's test\n",
    "stat_dagostino, p_dagostino = normaltest(ingresos_data)\n",
    "print(f\"\\nD'Agostino and Pearson's Test:\")\n",
    "print(f\"  Estadístico: {stat_dagostino:.4f}\")\n",
    "print(f\"  p-valor: {p_dagostino:.4f}\")\n",
    "\n",
    "if p_dagostino < 0.05:\n",
    "    print(f\"  Resultado: RECHAZAMOS H0 (p < 0.05)\")\n",
    "    print(f\"  Conclusión: Los ingresos NO siguen una distribución normal\")\n",
    "else:\n",
    "    print(f\"  Resultado: NO rechazamos H0 (p ≥ 0.05)\")\n",
    "    print(f\"  Conclusión: No hay evidencia suficiente para rechazar la normalidad\")\n",
    "\n",
    "# Visualización Q-Q plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histograma con curva normal\n",
    "ax1.hist(ingresos_data, bins=30, density=True, alpha=0.7, edgecolor='black')\n",
    "xmin, xmax = ax1.get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, ingresos_data.mean(), ingresos_data.std())\n",
    "ax1.plot(x, p, 'r-', linewidth=2, label='Normal teórica')\n",
    "ax1.set_title('Distribución de Ingresos vs Normal')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(ingresos_data, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot - Ingresos vs Normal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipótesis 2: Existe diferencia significativa en ingresos por nivel educativo\n",
    "print(\"\\n--- HIPÓTESIS 2: Diferencia de Ingresos por Educación ---\")\n",
    "print(\"H0: No hay diferencia significativa en ingresos entre niveles educativos\")\n",
    "print(\"H1: Sí hay diferencia significativa en ingresos entre niveles educativos\")\n",
    "\n",
    "# Agrupar datos por educación\n",
    "grupos_educacion = []\n",
    "labels_educacion = []\n",
    "\n",
    "for nivel in df_clean['educacion'].cat.categories:\n",
    "    grupo_data = df_clean[df_clean['educacion'] == nivel]['ingresos'].dropna()\n",
    "    if len(grupo_data) > 0:\n",
    "        grupos_educacion.append(grupo_data)\n",
    "        labels_educacion.append(nivel)\n",
    "\n",
    "# ANOVA de una vía\n",
    "if len(grupos_educacion) > 2:\n",
    "    stat_anova, p_anova = stats.f_oneway(*grupos_educacion)\n",
    "    print(f\"\\nANOVA de una vía:\")\n",
    "    print(f\"  Estadístico F: {stat_anova:.4f}\")\n",
    "    print(f\"  p-valor: {p_anova:.4f}\")\n",
    "    \n",
    "    if p_anova < 0.05:\n",
    "        print(f\"  Resultado: RECHAZAMOS H0 (p < 0.05)\")\n",
    "        print(f\"  Conclusión: SÍ hay diferencia significativa en ingresos por educación\")\n",
    "    else:\n",
    "        print(f\"  Resultado: NO rechazamos H0 (p ≥ 0.05)\")\n",
    "        print(f\"  Conclusión: No hay evidencia suficiente de diferencias significativas\")\n",
    "\n",
    "# Estadísticas descriptivas por grupo\n",
    "print(\"\\nEstadísticas por nivel educativo:\")\n",
    "for i, (grupo, label) in enumerate(zip(grupos_educacion, labels_educacion)):\n",
    "    print(f\"  {label}: Media = {grupo.mean():.2f}, Mediana = {grupo.median():.2f}, n = {len(grupo)}\")\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_clean.boxplot(column='ingresos', by='educacion', ax=plt.gca())\n",
    "plt.title('Distribución de Ingresos por Nivel Educativo')\n",
    "plt.suptitle('')  # Remover título automático\n",
    "plt.xlabel('Nivel Educativo')\n",
    "plt.ylabel('Ingresos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipótesis 3: Existe asociación entre nivel educativo y categoría de cliente\n",
    "print(\"\\n--- HIPÓTESIS 3: Asociación Educación-Categoría Cliente ---\")\n",
    "print(\"H0: No hay asociación entre nivel educativo y categoría de cliente\")\n",
    "print(\"H1: Sí hay asociación entre nivel educativo y categoría de cliente\")\n",
    "\n",
    "# Crear tabla de contingencia\n",
    "contingency_table = pd.crosstab(df_clean['educacion'], df_clean['categoria_cliente'])\n",
    "print(\"\\nTabla de Contingencia:\")\n",
    "display(contingency_table)\n",
    "\n",
    "# Test Chi-cuadrado\n",
    "chi2_stat, p_chi2, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nTest Chi-cuadrado de independencia:\")\n",
    "print(f\"  Estadístico Chi-cuadrado: {chi2_stat:.4f}\")\n",
    "print(f\"  p-valor: {p_chi2:.4f}\")\n",
    "print(f\"  Grados de libertad: {dof}\")\n",
    "\n",
    "if p_chi2 < 0.05:\n",
    "    print(f\"  Resultado: RECHAZAMOS H0 (p < 0.05)\")\n",
    "    print(f\"  Conclusión: SÍ hay asociación significativa entre educación y categoría\")\n",
    "else:\n",
    "    print(f\"  Resultado: NO rechazamos H0 (p ≥ 0.05)\")\n",
    "    print(f\"  Conclusión: No hay evidencia suficiente de asociación\")\n",
    "\n",
    "# Visualización de la tabla de contingencia\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Tabla de Contingencia: Educación vs Categoría Cliente')\n",
    "plt.ylabel('Nivel Educativo')\n",
    "plt.xlabel('Categoría Cliente')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar frecuencias esperadas\n",
    "print(\"\\nFrecuencias esperadas bajo H0:\")\n",
    "expected_df = pd.DataFrame(expected, \n",
    "                          index=contingency_table.index, \n",
    "                          columns=contingency_table.columns)\n",
    "display(expected_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning\n",
    "\n",
    "Entrenamos cinco modelos de Machine Learning y evaluamos su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PREPARACIÓN DE DATOS PARA MACHINE LEARNING ===\")\n",
    "\n",
    "# Definir variable objetivo (para este ejemplo, predeciremos la categoría de cliente)\n",
    "target_column = 'categoria_cliente'\n",
    "feature_columns = [col for col in df_clean.columns if col != target_column]\n",
    "\n",
    "print(f\"Variable objetivo: {target_column}\")\n",
    "print(f\"Variables predictoras: {feature_columns}\")\n",
    "\n",
    "# Preparar características (X) y variable objetivo (y)\n",
    "X = df_clean[feature_columns].copy()\n",
    "y = df_clean[target_column].copy()\n",
    "\n",
    "# Codificar variables categóricas\n",
    "label_encoders = {}\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Variable '{col}' codificada\")\n",
    "\n",
    "# Codificar variable objetivo\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nClases en variable objetivo: {target_encoder.classes_}\")\n",
    "print(f\"Distribución de clases:\")\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "for i, (clase, count) in enumerate(zip(target_encoder.classes_, counts)):\n",
    "    print(f\"  {clase}: {count} ({count/len(y_encoded)*100:.1f}%)\")\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTamaño del conjunto de entrenamiento: {X_train.shape[0]}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]}\")\n",
    "\n",
    "# Normalizar características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nDatos preparados exitosamente para ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los cinco modelos de Machine Learning\n",
    "print(\"=== ENTRENAMIENTO DE MODELOS DE MACHINE LEARNING ===\")\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "results = {}\n",
    "\n",
    "print(\"Entrenando modelos...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    \n",
    "    # Decidir si usar datos escalados o no\n",
    "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "        X_train_use = X_train_scaled\n",
    "        X_test_use = X_test_scaled\n",
    "        print(\"  Usando datos escalados\")\n",
    "    else:\n",
    "        X_train_use = X_train\n",
    "        X_test_use = X_test\n",
    "        print(\"  Usando datos originales\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_use)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)\n",
    "    \n",
    "    # Métricas de evaluación\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Validación cruzada\n",
    "    cv_scores = cross_val_score(model, X_train_use, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # AUC-ROC (para problemas multiclase, usar macro average)\n",
    "    try:\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "    except:\n",
    "        auc_score = 'N/A'\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'auc_score': auc_score,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    print(f\"  AUC Score: {auc_score if auc_score != 'N/A' else 'N/A'}\")\n",
    "    print()\n",
    "\n",
    "print(\"Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación comparativa de modelos\n",
    "print(\"=== EVALUACIÓN COMPARATIVA DE MODELOS ===\")\n",
    "\n",
    "# Crear tabla de comparación\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Modelo': name,\n",
    "        'Accuracy': f\"{result['accuracy']:.4f}\",\n",
    "        'CV Mean': f\"{result['cv_mean']:.4f}\",\n",
    "        'CV Std': f\"{result['cv_std']:.4f}\",\n",
    "        'AUC Score': f\"{result['auc_score']:.4f}\" if result['auc_score'] != 'N/A' else 'N/A'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# Identificar el mejor modelo\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['cv_mean'])\n",
    "print(f\"\\nMejor modelo basado en CV Score: {best_model_name}\")\n",
    "print(f\"CV Score: {results[best_model_name]['cv_mean']:.4f}\")\n",
    "\n",
    "# Visualización comparativa\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Gráfico de barras - Accuracy\n",
    "models_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in models_names]\n",
    "cv_means = [results[name]['cv_mean'] for name in models_names]\n",
    "\n",
    "x = np.arange(len(models_names))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8)\n",
    "ax1.bar(x + width/2, cv_means, width, label='CV Mean', alpha=0.8)\n",
    "ax1.set_xlabel('Modelos')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Comparación de Accuracy y CV Score')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_names, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot de CV scores\n",
    "cv_scores_all = []\n",
    "model_labels = []\n",
    "for name, model in models.items():\n",
    "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "        X_use = X_train_scaled\n",
    "    else:\n",
    "        X_use = X_train\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_use, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores_all.append(cv_scores)\n",
    "    model_labels.append(name)\n",
    "\n",
    "ax2.boxplot(cv_scores_all, labels=model_labels)\n",
    "ax2.set_title('Distribución de CV Scores')\n",
    "ax2.set_ylabel('CV Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis detallado del mejor modelo\n",
    "print(f\"=== ANÁLISIS DETALLADO DEL MEJOR MODELO: {best_model_name} ===\")\n",
    "\n",
    "best_model = results[best_model_name]['model']\n",
    "best_y_pred = results[best_model_name]['y_pred']\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "class_names = target_encoder.classes_\n",
    "print(classification_report(y_test, best_y_pred, target_names=class_names))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, best_y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Matriz de Confusión - {best_model_name}')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.xlabel('Predicción')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Importancia de características (si está disponible)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\nImportancia de Características:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    display(feature_importance)\n",
    "    \n",
    "    # Visualización de importancia\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance.head(10), x='Importance', y='Feature')\n",
    "    plt.title(f'Top 10 Características Más Importantes - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    print(\"\\nCoeficientes del Modelo:\")\n",
    "    # Para modelos lineales multiclase\n",
    "    if len(best_model.coef_.shape) > 1:\n",
    "        coef_df = pd.DataFrame(best_model.coef_.T, \n",
    "                              index=feature_columns, \n",
    "                              columns=class_names)\n",
    "        display(coef_df)\n",
    "    else:\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Coefficient': best_model.coef_[0]\n",
    "        }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "        display(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final del proyecto\n",
    "print(\"=== RESUMEN FINAL DEL PROYECTO ===\")\n",
    "print(\"\\n1. EXTRACCIÓN DE DATOS:\")\n",
    "print(f\"   - Dataset inicial: {df_original.shape[0]} filas, {df_original.shape[1]} columnas\")\n",
    "print(f\"   - Dataset limpio: {df_clean.shape[0]} filas, {df_clean.shape[1]} columnas\")\n",
    "\n",
    "print(\"\\n2. LIMPIEZA DE DATOS:\")\n",
    "print(f\"   - Valores nulos tratados: {df_original.isnull().sum().sum()}\")\n",
    "print(f\"   - Duplicados eliminados: {df_original.shape[0] - df_clean.shape[0]}\")\n",
    "print(f\"   - Variables categóricas codificadas: {len(categorical_features)}\")\n",
    "\n",
    "print(\"\\n3. ANÁLISIS EXPLORATORIO:\")\n",
    "print(f\"   - Variables numéricas analizadas: {len(numerical_cols)}\")\n",
    "print(f\"   - Variables categóricas analizadas: {len(categorical_cols)}\")\n",
    "print(f\"   - Correlaciones fuertes encontradas: {len(strong_correlations)}\")\n",
    "\n",
    "print(\"\\n4. ANÁLISIS ESTADÍSTICO:\")\n",
    "print(\"   - 3 hipótesis planteadas y evaluadas\")\n",
    "print(\"   - Tests de normalidad, ANOVA y Chi-cuadrado realizados\")\n",
    "\n",
    "print(\"\\n5. MACHINE LEARNING:\")\n",
    "print(f\"   - Modelos entrenados: {len(models)}\")\n",
    "print(f\"   - Mejor modelo: {best_model_name}\")\n",
    "print(f\"   - Mejor CV Score: {results[best_model_name]['cv_mean']:.4f}\")\n",
    "print(f\"   - Accuracy en test: {results[best_model_name]['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n=== PROYECTO COMPLETADO EXITOSAMENTE ===\")\n",
    "\n",
    "# Guardar resultados finales\n",
    "final_results = {\n",
    "    'dataset_shape': df_clean.shape,\n",
    "    'best_model': best_model_name,\n",
    "    'best_accuracy': results[best_model_name]['accuracy'],\n",
    "    'best_cv_score': results[best_model_name]['cv_mean'],\n",
    "    'all_models_performance': {name: result['cv_mean'] for name, result in results.items()}\n",
    "}\n",
    "\n",
    "print(\"\\nResultados finales guardados en la variable 'final_results'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}